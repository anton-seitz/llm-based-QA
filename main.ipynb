{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "CqzqyE5jwz0n",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqzqyE5jwz0n",
        "outputId": "5df13ffb-fe53-4f05-c447-935706ecd77a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://anu9rng:****@rb-artifactory.bosch.com/artifactory/api/pypi/python-virtual/simple\n",
            "Requirement already satisfied: transformers in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.26.3)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2023.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\sea4sgm\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2023.11.17)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Looking in indexes: https://anu9rng:****@rb-artifactory.bosch.com/artifactory/api/pypi/python-virtual/simple\n",
            "Requirement already satisfied: thefuzz in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.22.1)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.0.0 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thefuzz) (3.13.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Looking in indexes: https://anu9rng:****@rb-artifactory.bosch.com/artifactory/api/pypi/python-virtual/simpleNote: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Requirement already satisfied: fuzzywuzzy in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.18.0)\n",
            "Looking in indexes: https://anu9rng:****@rb-artifactory.bosch.com/artifactory/api/pypi/python-virtual/simple\n",
            "Requirement already satisfied: transformers in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.51.3)\n",
            "Requirement already satisfied: datasets in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.6.0)\n",
            "Requirement already satisfied: peft in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.15.2)\n",
            "Requirement already satisfied: accelerate in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.6.0)\n",
            "Requirement already satisfied: bitsandbytes in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.45.5)\n",
            "Requirement already satisfied: filelock in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.26.3)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2023.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (20.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.9.1)\n",
            "Requirement already satisfied: psutil in c:\\users\\sea4sgm\\appdata\\roaming\\python\\python311\\site-packages (from peft) (6.1.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from peft) (2.1.2)\n",
            "Requirement already satisfied: sympy in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.13.0->peft) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.13.0->peft) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.0 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: colorama in c:\\users\\sea4sgm\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Looking in indexes: https://anu9rng:****@rb-artifactory.bosch.com/artifactory/api/pypi/python-virtual/simple\n",
            "Requirement already satisfied: sentence-transformers in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (4.51.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (2.1.2)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (1.4.0)\n",
            "Requirement already satisfied: scipy in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (0.31.2)\n",
            "Requirement already satisfied: Pillow in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (10.2.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.3)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.12.2)\n",
            "Requirement already satisfied: sympy in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\sea4sgm\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.11.17)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers\n",
        "%pip install thefuzz\n",
        "%pip install fuzzywuzzy\n",
        "%pip install transformers datasets peft accelerate bitsandbytes\n",
        "%pip install --upgrade sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19f09b13",
      "metadata": {},
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- https://en.wikipedia.org/wiki/Judo\n",
        "\n",
        "- https://en.wikipedia.org/wiki/List_of_judo_techniques\n",
        "\n",
        "- https://en.wikipedia.org/wiki/List_of_judoka\n",
        "\n",
        "- https://martialarts.fandom.com/wiki/Judo\n",
        "\n",
        "- https://chas-ma.com/JudoManual/Chapter_2%28HistoryofJudo%29.pdf\n",
        "\n",
        "- https://www.ijf.org/history\n",
        "\n",
        "- https://blackbelttrek.com/judo-vs-jiu-jitsu-the-ultimate-comparison/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "28db04e8",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from transformers import pipeline\n",
        "\n",
        "# Lade das vortrainierte Modell\n",
        "model_name = \"deepset/roberta-base-squad2\"\n",
        "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
        "\n",
        "data_dir = \"data-sources\"\n",
        "context_file = \"complete_context.txt\"\n",
        "\n",
        "# Lese alle .txt-Dateien im Verzeichnis data-sources ein\n",
        "context_parts = []\n",
        "for filename in os.listdir(data_dir):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        file_path = os.path.join(data_dir, filename)\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            context_parts.append(f.read())\n",
        "\n",
        "# Verbinde alle Texte zu einem Gesamttext\n",
        "context = \"\\n\".join(context_parts)\n",
        "\n",
        "# Speichere den gesamten Kontext in eine Datei\n",
        "with open(context_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(context)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fba29520",
      "metadata": {},
      "source": [
        "# Funktion zum Evaluieren"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7ff20666",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load a pre-trained SBERT model (only once)\n",
        "sbert = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def evaluate_qa(data: list,\n",
        "                context: str,\n",
        "                nlp_callable,\n",
        "                threshold: float) -> float:\n",
        "    correct = 0                     # initialize correct count\n",
        "    total = len(data)               # total questions\n",
        "\n",
        "    for item in data:\n",
        "        question = item[\"question\"] \n",
        "        expected = item[\"answer\"]\n",
        "        QA_input = {\"question\": question, \"context\": context}  # model input\n",
        "\n",
        "        result = nlp_callable(QA_input)   # run QA pipeline\n",
        "        pred = result[\"answer\"]           # predicted answer\n",
        "\n",
        "        # Extract answer span from result\n",
        "        start = result.get(\"start\", None)\n",
        "        end = result.get(\"end\", None)\n",
        "        span_text = context[start:end] if start is not None and end is not None else \"N/A\"\n",
        "\n",
        "        # --- Semantic similarity via SBERT ---\n",
        "        emb_pred = sbert.encode(pred, convert_to_tensor=True)      # embed prediction\n",
        "        emb_exp  = sbert.encode(expected, convert_to_tensor=True)  # embed expected\n",
        "        sim_score = util.cos_sim(emb_pred, emb_exp).item() * 100   # cosine sim ×100\n",
        "\n",
        "        status = \"✅\" if sim_score >= threshold else \"❌\"  # check threshold\n",
        "        print(f\"{status} Question: {question}\")\n",
        "        print(f\"   Expected: {expected}         Received: {pred}\")\n",
        "        print(f\"   Span: '{span_text}' (start: {start}, end: {end})\\n\")\n",
        "        print(f\"   Similarity Score: {round(sim_score, 2)}\\n\")\n",
        "\n",
        "        if sim_score >= threshold:\n",
        "            correct += 1  # count as correct\n",
        "\n",
        "    accuracy = (correct / total) * 100 if total else 0   # compute accuracy\n",
        "    print(f\"\\n✅ Accuracy: {accuracy:.2f}% ({correct}/{total} correct)\")\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6c03b4f3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context length (chars):  141653\n",
            "Avg Token length: 2.93\n",
            "Total Tokens: 41611\n"
          ]
        }
      ],
      "source": [
        "from transformers import RobertaTokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "# Ensure context is loaded\n",
        "if 'context' not in locals():\n",
        "\twith open(context_file, \"r\", encoding=\"utf-8\") as f:\n",
        "\t\tcontext = f.read()\n",
        "print('Context length (chars): ', len(context))\n",
        "\n",
        "tokens = tokenizer.tokenize(context)\n",
        "\n",
        "\n",
        "# Calculate total token length\n",
        "avg_length = sum(len(token.strip('Ġ')) for token in tokens)/len(tokens)\n",
        "print(f\"Avg Token length: {avg_length:.2f}\")\n",
        "print(f\"Total Tokens: {len(tokens)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d9902b5",
      "metadata": {},
      "source": [
        "# Basic Pipeline ausführen\n",
        "### Fragen aus Datensatz auswählen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "84eac78f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total questions: 78\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "# Load questions\n",
        "with open(\"question-sets/q_v3_rated.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    questions_data = json.load(f)\n",
        "easy_q = questions_data['easy']\n",
        "medium_q = questions_data['medium']\n",
        "hard_q = questions_data['hard']\n",
        "\n",
        "# Select random questions\n",
        "random.seed(0)\n",
        "sampled_easy = random.sample(easy_q, 15)\n",
        "sampled_medium = random.sample(medium_q, 15)\n",
        "sampled_hard = random.sample(hard_q, 15)\n",
        "\n",
        "# 1) Load all evaluation questions\n",
        "all_questions = easy_q + medium_q + hard_q\n",
        "print(f\"Total questions: {len(all_questions)}\")\n",
        "\n",
        "sampled_all = random.sample(all_questions, 15)\n",
        "THRESHOLD = 80"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3b6c8d8",
      "metadata": {
        "id": "b3b6c8d8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n",
            "c:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\question_answering.py:390: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "❌ Question: What does judo mean?\n",
            "   Expected: gentle way         Received: kappo\n",
            "   Span: 'kappo' (start: 21806, end: 21811)\n",
            "\n",
            "   Similarity Score: 11.86\n",
            "\n",
            "✅ Question: Who founded judo?\n",
            "   Expected: Kanō Jigorō         Received: Kano Jigoro\n",
            "   Span: 'Kano Jigoro' (start: 5059, end: 5070)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "❌ Question: What is the name of the school Kanō Jigorō established?\n",
            "   Expected: Kōdōkan         Received: Kōdōkan Judo Institute\n",
            "   Span: 'Kōdōkan Judo Institute' (start: 5176, end: 5198)\n",
            "\n",
            "   Similarity Score: 64.07\n",
            "\n",
            "✅ Question: What is the judo uniform called?\n",
            "   Expected: jūdōgi         Received: Judogi\n",
            "   Span: 'Judogi' (start: 58244, end: 58250)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "✅ Question: What color belt do novices wear?\n",
            "   Expected: white         Received: White\n",
            "   Span: 'White' (start: 1023, end: 1028)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "✅ Question: What color belt do masters wear?\n",
            "   Expected: black         Received: black\n",
            "   Span: 'black' (start: 1059, end: 1064)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "✅ Question: What is the term for judo students?\n",
            "   Expected: jūdōka         Received: Jūdōka\n",
            "   Span: 'Jūdōka' (start: 1128, end: 1134)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "✅ Question: What is the term for free practice in judo?\n",
            "   Expected: randori         Received: randori\n",
            "   Span: 'randori' (start: 17820, end: 17827)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "❌ Question: What is the objective of judo?\n",
            "   Expected: throw, pin, or submit opponent         Received: free practice\n",
            "   Span: 'free practice' (start: 17829, end: 17842)\n",
            "\n",
            "   Similarity Score: 24.82\n",
            "\n",
            "✅ Question: What does ippon mean?\n",
            "   Expected: one point         Received: one point\n",
            "   Span: 'one point' (start: 3452, end: 3461)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "✅ Question: What does waza-ari mean?\n",
            "   Expected: half point         Received: half point\n",
            "   Span: 'half point' (start: 3550, end: 3560)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "✅ Question: What is a minor point called?\n",
            "   Expected: yuko         Received: yuko\n",
            "   Span: 'yuko' (start: 3588, end: 3592)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "✅ Question: What is the category for standing techniques?\n",
            "   Expected: tachi-waza         Received: tachi-waza\n",
            "   Span: 'tachi-waza' (start: 69654, end: 69664)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "✅ Question: What is the category for ground techniques?\n",
            "   Expected: ne-waza         Received: ne-waza\n",
            "   Span: 'ne-waza' (start: 21956, end: 21963)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "❌ Question: Who is the person performing the throw?\n",
            "   Expected: tori         Received: judoka\n",
            "   Span: 'judoka' (start: 4912, end: 4918)\n",
            "\n",
            "   Similarity Score: 28.81\n",
            "\n",
            "❌ Question: Who is the person receiving the throw?\n",
            "   Expected: uke         Received: philosophy\n",
            "   Span: 'philosophy' (start: 47895, end: 47905)\n",
            "\n",
            "   Similarity Score: 14.86\n",
            "\n",
            "❌ Question: Name a shime-waza technique.\n",
            "   Expected: Nami-juji-jime         Received: throwing techniques\n",
            "   Span: 'throwing techniques' (start: 93587, end: 93606)\n",
            "\n",
            "   Similarity Score: 2.41\n",
            "\n",
            "❌ Question: Name a kansetsu-waza technique.\n",
            "   Expected: Ude-garami         Received: throwing\n",
            "   Span: 'throwing' (start: 18474, end: 18482)\n",
            "\n",
            "   Similarity Score: 11.55\n",
            "\n",
            "❌ Question: Name an osaekomi-waza technique.\n",
            "   Expected: Kesa-gatame         Received: throwing\n",
            "   Span: 'throwing' (start: 18474, end: 18482)\n",
            "\n",
            "   Similarity Score: 12.62\n",
            "\n",
            "❌ Question: What type of contact is judo?\n",
            "   Expected: full contact         Received: mind-body\n",
            "   Span: 'mind-body' (start: 45421, end: 45430)\n",
            "\n",
            "   Similarity Score: 19.92\n",
            "\n",
            "❌ Question: Is judo mixed-sex?\n",
            "   Expected: no         Received: Mixed-sex\n",
            "   Span: 'Mixed-sex' (start: 59806, end: 59815)\n",
            "\n",
            "   Similarity Score: 18.51\n",
            "\n",
            "❌ Question: What is the focus of judo?\n",
            "   Expected: throwing         Received: hip throws\n",
            "   Span: 'hip throws' (start: 91437, end: 91447)\n",
            "\n",
            "   Similarity Score: 61.36\n",
            "\n",
            "❌ Question: What does judogi translate to?\n",
            "   Expected: judo attire         Received: uniform\n",
            "   Span: 'uniform' (start: 58252, end: 58259)\n",
            "\n",
            "   Similarity Score: 45.15\n",
            "\n",
            "❌ Question: What does judoka mean?\n",
            "   Expected: judo performer         Received: judo practitioners\n",
            "   Span: 'judo practitioners' (start: 115961, end: 115979)\n",
            "\n",
            "   Similarity Score: 79.23\n",
            "\n",
            "✅ Question: What does nage-waza include?\n",
            "   Expected: throwing techniques         Received: throwing techniques\n",
            "   Span: 'throwing techniques' (start: 18474, end: 18493)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "✅ Question: What is the governing body of international judo?\n",
            "   Expected: International Judo Federation         Received: International Judo Federation\n",
            "   Span: 'International Judo Federation' (start: 1737, end: 1766)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "✅ Question: What do judoka wear on their feet during practice?\n",
            "   Expected: bare feet         Received: bare feet\n",
            "   Span: 'bare feet' (start: 1177, end: 1186)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "❌ Question: What is the traditional judo attire made of?\n",
            "   Expected: strong white cloth         Received: kimono\n",
            "   Span: 'kimono' (start: 100679, end: 100685)\n",
            "\n",
            "   Similarity Score: 20.56\n",
            "\n",
            "\n",
            "✅ Accuracy: 50.00% (14/28 correct)\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = len(easy_q)\n",
        "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
        "\"\"\"\n",
        "# Run evaluation\n",
        "for item in easy_q:\n",
        "    question = item[\"question\"]\n",
        "    expected_answer = item[\"answer\"]\n",
        "    QA_input = {\n",
        "        \"question\": question,\n",
        "        \"context\": context  \n",
        "    }\n",
        "\"\"\"\n",
        "ACC_FullContext = evaluate_qa(\n",
        "    easy_q,  # <- data (list of QA dicts)\n",
        "    context,            # <- context string\n",
        "    nlp,                # <- your pipeline/callable\n",
        "    THRESHOLD           # <- int threshold\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bedeba8",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "e6a32c65",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\question_answering.py:390: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Question: What is the term for pre-arranged forms in judo?\n",
            "   Expected: kata         Received: kata\n",
            "   Span: 'kata' (start: 61084, end: 61088)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "❌ Question: From which martial art did judo originate?\n",
            "   Expected: jujitsu         Received: jiu-jitsu\n",
            "   Span: 'jiu-jitsu' (start: 44523, end: 44532)\n",
            "\n",
            "   Similarity Score: 67.01\n",
            "\n",
            "✅ Question: What is the Japanese term for throwing techniques?\n",
            "   Expected: nage-waza         Received: nage-waza\n",
            "   Span: 'nage-waza' (start: 64975, end: 64984)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "❌ Question: What is the category for sacrifice throws?\n",
            "   Expected: sutemi-waza         Received: nage waza\n",
            "   Span: 'nage waza' (start: 9353, end: 9362)\n",
            "\n",
            "   Similarity Score: 57.74\n",
            "\n",
            "✅ Question: What is the category for hip throwing techniques?\n",
            "   Expected: koshi-waza         Received: Koshi-waza\n",
            "   Span: 'Koshi-waza' (start: 23626, end: 23636)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "❌ Question: What is the category for foot throwing techniques?\n",
            "   Expected: ashi-waza         Received: nage waza\n",
            "   Span: 'nage waza' (start: 9353, end: 9362)\n",
            "\n",
            "   Similarity Score: 61.76\n",
            "\n",
            "❌ Question: What is the maximum dan rank in judo?\n",
            "   Expected: 10th dan         Received: 10th\n",
            "   Span: '10th' (start: 131649, end: 131653)\n",
            "\n",
            "   Similarity Score: 73.45\n",
            "\n",
            "✅ Question: What is the symbol at the center of the Kodokan emblem?\n",
            "   Expected: red circle         Received: red circle\n",
            "   Span: 'red circle' (start: 9739, end: 9749)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "❌ Question: What black belt rank is shodan?\n",
            "   Expected: first rank         Received: lowest\n",
            "   Span: 'lowest' (start: 6890, end: 6896)\n",
            "\n",
            "   Similarity Score: 45.43\n",
            "\n",
            "❌ Question: Name a Kodokan kata.\n",
            "   Expected: Ju-no-kata         Received: Koshi-jime\n",
            "   Span: 'Koshi-jime' (start: 28922, end: 28932)\n",
            "\n",
            "   Similarity Score: 38.84\n",
            "\n",
            "✅ Question: What does katame-waza include?\n",
            "   Expected: grappling techniques         Received: grappling techniques\n",
            "   Span: 'grappling techniques' (start: 22137, end: 22157)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "✅ Question: What did Kano eliminate from his art?\n",
            "   Expected: the most dangerous techniques         Received: the most dangerous techniques\n",
            "   Span: 'the most dangerous techniques' (start: 1459, end: 1488)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "❌ Question: What did Kano preserve in kata?\n",
            "   Expected: classical techniques of jujitsu         Received: prearranged forms\n",
            "   Span: 'prearranged forms' (start: 93227, end: 93244)\n",
            "\n",
            "   Similarity Score: 9.57\n",
            "\n",
            "❌ Question: What influenced European and Russian judoka?\n",
            "   Expected: their strong wrestling traditions         Received: traditional forms of combat\n",
            "   Span: 'traditional forms of combat' (start: 7039, end: 7066)\n",
            "\n",
            "   Similarity Score: 28.51\n",
            "\n",
            "✅ Question: What Russian art was based on judo?\n",
            "   Expected: sambo         Received: sambo\n",
            "   Span: 'sambo' (start: 3999, end: 4004)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "❌ Question: Which American judoka is also an MMA fighter?\n",
            "   Expected: Ronda Rousey         Received: Hidehiko Yoshida\n",
            "   Span: 'Hidehiko Yoshida' (start: 133357, end: 133373)\n",
            "\n",
            "   Similarity Score: 29.7\n",
            "\n",
            "✅ Question: How many national federations does the IJF bring together?\n",
            "   Expected: more than 200         Received: more than 200\n",
            "   Span: 'more than 200' (start: 7466, end: 7479)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "✅ Question: How many continental unions does the IJF have?\n",
            "   Expected: 5         Received: 5\n",
            "   Span: '5' (start: 7505, end: 7506)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "✅ Question: Where is the International Judo Federation headquartered?\n",
            "   Expected: Budapest, Hungary         Received: Budapest, Hungary\n",
            "   Span: 'Budapest, Hungary' (start: 1794, end: 1811)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "\n",
            "✅ Accuracy: 52.63% (10/19 correct)\n"
          ]
        }
      ],
      "source": [
        "ACC_FullContext = evaluate_qa(\n",
        "    medium_q,  # <- data (list of QA dicts)\n",
        "    context,            # <- context string\n",
        "    nlp,                # <- your pipeline/callable\n",
        "    THRESHOLD           # <- int threshold\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aacdb616",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Question: In what year was judo founded?\n",
            "   Expected: 1882         Received: 1882\n",
            "   Span: '1882' (start: 60140, end: 60144)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "✅ Question: What is the Japanese term for grappling techniques?\n",
            "   Expected: katame-waza         Received: Katame-waza\n",
            "   Span: 'Katame-waza' (start: 21225, end: 21236)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "✅ Question: What is the Japanese term for body-striking techniques?\n",
            "   Expected: atemi-waza         Received: Atemi-waza\n",
            "   Span: 'Atemi-waza' (start: 21265, end: 21275)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "✅ Question: What is the Japanese term for blocks and parries?\n",
            "   Expected: uke-waza         Received: Uke-waza\n",
            "   Span: 'Uke-waza' (start: 33762, end: 33770)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "✅ Question: What is the Japanese term for resuscitation techniques?\n",
            "   Expected: kappo         Received: Kappo\n",
            "   Span: 'Kappo' (start: 21415, end: 21420)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "❌ Question: How many throws are in the Kodokan Gokyo-no-waza?\n",
            "   Expected: 67         Received: philosophy that drives the two fighting styles\n",
            "   Span: 'philosophy that drives the two fighting styles' (start: 47895, end: 47941)\n",
            "\n",
            "   Similarity Score: -3.12\n",
            "\n",
            "✅ Question: When did men's judo first appear at the Olympics?\n",
            "   Expected: 1964         Received: 1964\n",
            "   Span: '1964' (start: 2036, end: 2040)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "✅ Question: When did women's judo first appear at the Olympics?\n",
            "   Expected: 1992         Received: 1992\n",
            "   Span: '1992' (start: 2167, end: 2171)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "✅ Question: In what year did the first women's World Judo Championships take place?\n",
            "   Expected: 1980         Received: 1980\n",
            "   Span: '1980' (start: 2120, end: 2124)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "✅ Question: In what year did women's judo debut as a demonstration sport at the Olympics?\n",
            "   Expected: 1988         Received: 1988\n",
            "   Span: '1988' (start: 80952, end: 80956)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "❌ Question: What are the two guiding principles of judo?\n",
            "   Expected: Seiryoku‑Zen'yō and Jita‑Kyōei         Received: life, art and science\n",
            "   Span: 'life, art and science' (start: 79065, end: 79086)\n",
            "\n",
            "   Similarity Score: 15.01\n",
            "\n",
            "❌ Question: What does Seiryoku‑Zen'yō mean?\n",
            "   Expected: maximum efficient use of energy         Received: maximum efficiency, minimum effort\n",
            "   Span: 'maximum efficiency, minimum effort' (start: 19509, end: 19543)\n",
            "\n",
            "   Similarity Score: 68.87\n",
            "\n",
            "✅ Question: What does Jita‑Kyōei mean?\n",
            "   Expected: mutual welfare and benefit         Received: mutual welfare and benefit\n",
            "   Span: 'mutual welfare and benefit' (start: 66174, end: 66200)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "✅ Question: In what year was the International Judo Federation founded?\n",
            "   Expected: 1951         Received: 1951\n",
            "   Span: '1951' (start: 98291, end: 98295)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "✅ Question: Who was the first president of the International Judo Federation?\n",
            "   Expected: Aldo Torti         Received: Aldo Torti\n",
            "   Span: 'Aldo Torti' (start: 12908, end: 12918)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "❌ Question: Who succeeded Aldo Torti as IJF president?\n",
            "   Expected: Risei Kano         Received: philosophy\n",
            "   Span: 'philosophy' (start: 47895, end: 47905)\n",
            "\n",
            "   Similarity Score: 17.44\n",
            "\n",
            "✅ Question: What shape is the Kodokan emblem?\n",
            "   Expected: octagonal mirror         Received: octagonal mirror\n",
            "   Span: 'octagonal mirror' (start: 9688, end: 9704)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "❌ Question: What was the initial dojo site in Tokyo founded by Kano?\n",
            "   Expected: Eisho-ji         Received: Kōdōkan Judo Institute\n",
            "   Span: 'Kōdōkan Judo Institute' (start: 5176, end: 5198)\n",
            "\n",
            "   Similarity Score: 23.0\n",
            "\n",
            "✅ Question: Which two students received the first ever shodan ranks?\n",
            "   Expected: Tsunejirō Tomita and Shiro Saigo         Received: Tomita Tsunejirō and Shiro Saigo\n",
            "   Span: 'Tomita Tsunejirō and Shiro Saigo' (start: 6773, end: 6805)\n",
            "\n",
            "   Similarity Score: 98.92\n",
            "\n",
            "❌ Question: Which method added colored belts to denote grades in Europe?\n",
            "   Expected: Kawaishi Method         Received: philosophy\n",
            "   Span: 'philosophy' (start: 47895, end: 47905)\n",
            "\n",
            "   Similarity Score: 5.59\n",
            "\n",
            "❌ Question: Name a forbidden sacrifice throw in competition.\n",
            "   Expected: Kani basami         Received: Finger, toe and ankle locks\n",
            "   Span: 'Finger, toe and ankle locks' (start: 77790, end: 77817)\n",
            "\n",
            "   Similarity Score: 5.55\n",
            "\n",
            "❌ Question: Name a prohibited katame-waza technique.\n",
            "   Expected: Do-jime         Received: Daki age\n",
            "   Span: 'Daki age' (start: 23672, end: 23680)\n",
            "\n",
            "   Similarity Score: 10.34\n",
            "\n",
            "❌ Question: Name a non-Kodokan Japanese kata.\n",
            "   Expected: Go-no-sen-no-kata         Received: Anton Geesink\n",
            "   Span: 'Anton Geesink' (start: 120335, end: 120348)\n",
            "\n",
            "   Similarity Score: 18.95\n",
            "\n",
            "✅ Question: In what year did Kano open a women's section at the Kodokan?\n",
            "   Expected: 1923         Received: 1923\n",
            "   Span: '1923' (start: 11737, end: 11741)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "✅ Question: Who dedicated her life to spreading women's judo?\n",
            "   Expected: Keiko Fukuda         Received: Keiko Fukuda\n",
            "   Span: 'Keiko Fukuda' (start: 11942, end: 11954)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "✅ Question: In what year was the European Judo Union first created?\n",
            "   Expected: 1932         Received: 1932\n",
            "   Span: '1932' (start: 12204, end: 12208)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "✅ Question: Where were the first European Judo Championships held?\n",
            "   Expected: Dresden         Received: Dresden\n",
            "   Span: 'Dresden' (start: 12298, end: 12305)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "✅ Question: What does uke-waza include?\n",
            "   Expected: blocks and parries         Received: blocks and parries\n",
            "   Span: 'blocks and parries' (start: 21330, end: 21348)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "✅ Question: What does atemi-waza include?\n",
            "   Expected: body-striking techniques         Received: body-striking techniques\n",
            "   Span: 'body-striking techniques' (start: 21284, end: 21308)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "❌ Question: What did Kano stress in practice?\n",
            "   Expected: randori         Received: newaza\n",
            "   Span: 'newaza' (start: 87500, end: 87506)\n",
            "\n",
            "   Similarity Score: 33.91\n",
            "\n",
            "✅ Question: What English phrase describes ju yoku go o seisu?\n",
            "   Expected: softness controls hardness         Received: softness controls hardness\n",
            "   Span: 'softness controls hardness' (start: 19680, end: 19706)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "❌ Question: What was judo’s inclusion status for the 1940 Tokyo Olympics?\n",
            "   Expected: demonstration sport         Received: philosophy\n",
            "   Span: 'philosophy' (start: 47895, end: 47905)\n",
            "\n",
            "   Similarity Score: 13.27\n",
            "\n",
            "❌ Question: Which Olympic Games marked judo’s competitive transformation?\n",
            "   Expected: 1964 Tokyo Olympics         Received: Summer Olympic Games\n",
            "   Span: 'Summer Olympic Games' (start: 230, end: 250)\n",
            "\n",
            "   Similarity Score: 50.8\n",
            "\n",
            "\n",
            "✅ Accuracy: 63.64% (21/33 correct)\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "ACC_FullContext = evaluate_qa(\n",
        "    hard_q,  # <- data (list of QA dicts)\n",
        "    context,            # <- context string\n",
        "    nlp,                # <- your pipeline/callable\n",
        "    THRESHOLD           # <- int threshold\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22119ad0",
      "metadata": {},
      "source": [
        "# Textcorpus reduzieren\n",
        "### Korpus in Chunks teilen und nur die top_k relevantesten Chunks im Context behalten -> Schnellere Ausführung"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "bfa68286",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total chunks: 378\n",
            "Chunks ≥ 20 words: 215\n",
            "Computing chunk embeddings...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b2870711df934c45bdab364436a00049",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing question embeddings...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ff4b6965bc24739be4ff4266ddd7dd4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected top 161 chunks (by semantic relevance):\n",
            "  1. Chunk #7 — Cosine Score: 0.7320\n",
            "  2. Chunk #184 — Cosine Score: 0.7056\n",
            "  3. Chunk #110 — Cosine Score: 0.7023\n",
            "  4. Chunk #2 — Cosine Score: 0.7002\n",
            "  5. Chunk #189 — Cosine Score: 0.6954\n",
            "  6. Chunk #114 — Cosine Score: 0.6950\n",
            "  7. Chunk #197 — Cosine Score: 0.6914\n",
            "  8. Chunk #8 — Cosine Score: 0.6886\n",
            "  9. Chunk #22 — Cosine Score: 0.6832\n",
            "  10. Chunk #185 — Cosine Score: 0.6765\n",
            "  11. Chunk #6 — Cosine Score: 0.6760\n",
            "  12. Chunk #36 — Cosine Score: 0.6750\n",
            "  13. Chunk #38 — Cosine Score: 0.6740\n",
            "  14. Chunk #112 — Cosine Score: 0.6731\n",
            "  15. Chunk #199 — Cosine Score: 0.6723\n",
            "  16. Chunk #179 — Cosine Score: 0.6715\n",
            "  17. Chunk #5 — Cosine Score: 0.6693\n",
            "  18. Chunk #14 — Cosine Score: 0.6684\n",
            "  19. Chunk #214 — Cosine Score: 0.6609\n",
            "  20. Chunk #116 — Cosine Score: 0.6603\n",
            "  21. Chunk #1 — Cosine Score: 0.6590\n",
            "  22. Chunk #28 — Cosine Score: 0.6564\n",
            "  23. Chunk #166 — Cosine Score: 0.6517\n",
            "  24. Chunk #113 — Cosine Score: 0.6516\n",
            "  25. Chunk #60 — Cosine Score: 0.6516\n",
            "  26. Chunk #115 — Cosine Score: 0.6500\n",
            "  27. Chunk #141 — Cosine Score: 0.6473\n",
            "  28. Chunk #175 — Cosine Score: 0.6459\n",
            "  29. Chunk #188 — Cosine Score: 0.6450\n",
            "  30. Chunk #13 — Cosine Score: 0.6421\n",
            "  31. Chunk #196 — Cosine Score: 0.6379\n",
            "  32. Chunk #200 — Cosine Score: 0.6359\n",
            "  33. Chunk #183 — Cosine Score: 0.6348\n",
            "  34. Chunk #173 — Cosine Score: 0.6344\n",
            "  35. Chunk #140 — Cosine Score: 0.6303\n",
            "  36. Chunk #128 — Cosine Score: 0.6299\n",
            "  37. Chunk #40 — Cosine Score: 0.6298\n",
            "  38. Chunk #109 — Cosine Score: 0.6293\n",
            "  39. Chunk #150 — Cosine Score: 0.6285\n",
            "  40. Chunk #129 — Cosine Score: 0.6279\n",
            "  41. Chunk #54 — Cosine Score: 0.6270\n",
            "  42. Chunk #167 — Cosine Score: 0.6265\n",
            "  43. Chunk #201 — Cosine Score: 0.6261\n",
            "  44. Chunk #187 — Cosine Score: 0.6255\n",
            "  45. Chunk #178 — Cosine Score: 0.6241\n",
            "  46. Chunk #177 — Cosine Score: 0.6233\n",
            "  47. Chunk #62 — Cosine Score: 0.6212\n",
            "  48. Chunk #67 — Cosine Score: 0.6202\n",
            "  49. Chunk #190 — Cosine Score: 0.6183\n",
            "  50. Chunk #120 — Cosine Score: 0.6158\n",
            "  51. Chunk #194 — Cosine Score: 0.6146\n",
            "  52. Chunk #3 — Cosine Score: 0.6139\n",
            "  53. Chunk #165 — Cosine Score: 0.6138\n",
            "  54. Chunk #146 — Cosine Score: 0.6133\n",
            "  55. Chunk #9 — Cosine Score: 0.6109\n",
            "  56. Chunk #10 — Cosine Score: 0.6083\n",
            "  57. Chunk #172 — Cosine Score: 0.6016\n",
            "  58. Chunk #15 — Cosine Score: 0.6014\n",
            "  59. Chunk #23 — Cosine Score: 0.6013\n",
            "  60. Chunk #186 — Cosine Score: 0.6005\n",
            "  61. Chunk #198 — Cosine Score: 0.5949\n",
            "  62. Chunk #148 — Cosine Score: 0.5931\n",
            "  63. Chunk #95 — Cosine Score: 0.5925\n",
            "  64. Chunk #176 — Cosine Score: 0.5920\n",
            "  65. Chunk #68 — Cosine Score: 0.5913\n",
            "  66. Chunk #164 — Cosine Score: 0.5911\n",
            "  67. Chunk #93 — Cosine Score: 0.5903\n",
            "  68. Chunk #149 — Cosine Score: 0.5894\n",
            "  69. Chunk #142 — Cosine Score: 0.5880\n",
            "  70. Chunk #124 — Cosine Score: 0.5870\n",
            "  71. Chunk #174 — Cosine Score: 0.5869\n",
            "  72. Chunk #31 — Cosine Score: 0.5862\n",
            "  73. Chunk #169 — Cosine Score: 0.5854\n",
            "  74. Chunk #41 — Cosine Score: 0.5825\n",
            "  75. Chunk #46 — Cosine Score: 0.5820\n",
            "  76. Chunk #191 — Cosine Score: 0.5813\n",
            "  77. Chunk #83 — Cosine Score: 0.5785\n",
            "  78. Chunk #43 — Cosine Score: 0.5773\n",
            "  79. Chunk #138 — Cosine Score: 0.5770\n",
            "  80. Chunk #208 — Cosine Score: 0.5763\n",
            "  81. Chunk #61 — Cosine Score: 0.5756\n",
            "  82. Chunk #26 — Cosine Score: 0.5704\n",
            "  83. Chunk #12 — Cosine Score: 0.5698\n",
            "  84. Chunk #117 — Cosine Score: 0.5688\n",
            "  85. Chunk #71 — Cosine Score: 0.5675\n",
            "  86. Chunk #51 — Cosine Score: 0.5670\n",
            "  87. Chunk #42 — Cosine Score: 0.5668\n",
            "  88. Chunk #152 — Cosine Score: 0.5665\n",
            "  89. Chunk #145 — Cosine Score: 0.5664\n",
            "  90. Chunk #86 — Cosine Score: 0.5655\n",
            "  91. Chunk #182 — Cosine Score: 0.5633\n",
            "  92. Chunk #171 — Cosine Score: 0.5627\n",
            "  93. Chunk #155 — Cosine Score: 0.5627\n",
            "  94. Chunk #204 — Cosine Score: 0.5611\n",
            "  95. Chunk #69 — Cosine Score: 0.5588\n",
            "  96. Chunk #193 — Cosine Score: 0.5579\n",
            "  97. Chunk #72 — Cosine Score: 0.5550\n",
            "  98. Chunk #130 — Cosine Score: 0.5548\n",
            "  99. Chunk #202 — Cosine Score: 0.5531\n",
            "  100. Chunk #209 — Cosine Score: 0.5515\n",
            "  101. Chunk #168 — Cosine Score: 0.5512\n",
            "  102. Chunk #170 — Cosine Score: 0.5504\n",
            "  103. Chunk #47 — Cosine Score: 0.5500\n",
            "  104. Chunk #123 — Cosine Score: 0.5496\n",
            "  105. Chunk #192 — Cosine Score: 0.5469\n",
            "  106. Chunk #97 — Cosine Score: 0.5450\n",
            "  107. Chunk #17 — Cosine Score: 0.5444\n",
            "  108. Chunk #56 — Cosine Score: 0.5442\n",
            "  109. Chunk #16 — Cosine Score: 0.5440\n",
            "  110. Chunk #154 — Cosine Score: 0.5439\n",
            "  111. Chunk #4 — Cosine Score: 0.5423\n",
            "  112. Chunk #139 — Cosine Score: 0.5419\n",
            "  113. Chunk #153 — Cosine Score: 0.5380\n",
            "  114. Chunk #203 — Cosine Score: 0.5374\n",
            "  115. Chunk #33 — Cosine Score: 0.5362\n",
            "  116. Chunk #50 — Cosine Score: 0.5334\n",
            "  117. Chunk #158 — Cosine Score: 0.5331\n",
            "  118. Chunk #126 — Cosine Score: 0.5305\n",
            "  119. Chunk #30 — Cosine Score: 0.5282\n",
            "  120. Chunk #205 — Cosine Score: 0.5278\n",
            "  121. Chunk #180 — Cosine Score: 0.5269\n",
            "  122. Chunk #35 — Cosine Score: 0.5243\n",
            "  123. Chunk #107 — Cosine Score: 0.5241\n",
            "  124. Chunk #151 — Cosine Score: 0.5207\n",
            "  125. Chunk #77 — Cosine Score: 0.5194\n",
            "  126. Chunk #32 — Cosine Score: 0.5184\n",
            "  127. Chunk #108 — Cosine Score: 0.5176\n",
            "  128. Chunk #133 — Cosine Score: 0.5173\n",
            "  129. Chunk #136 — Cosine Score: 0.5147\n",
            "  130. Chunk #98 — Cosine Score: 0.5135\n",
            "  131. Chunk #57 — Cosine Score: 0.5133\n",
            "  132. Chunk #70 — Cosine Score: 0.5129\n",
            "  133. Chunk #64 — Cosine Score: 0.5119\n",
            "  134. Chunk #121 — Cosine Score: 0.5107\n",
            "  135. Chunk #66 — Cosine Score: 0.5099\n",
            "  136. Chunk #21 — Cosine Score: 0.5070\n",
            "  137. Chunk #137 — Cosine Score: 0.5057\n",
            "  138. Chunk #212 — Cosine Score: 0.5025\n",
            "  139. Chunk #55 — Cosine Score: 0.5017\n",
            "  140. Chunk #48 — Cosine Score: 0.5008\n",
            "  141. Chunk #19 — Cosine Score: 0.4993\n",
            "  142. Chunk #80 — Cosine Score: 0.4944\n",
            "  143. Chunk #111 — Cosine Score: 0.4934\n",
            "  144. Chunk #39 — Cosine Score: 0.4928\n",
            "  145. Chunk #89 — Cosine Score: 0.4912\n",
            "  146. Chunk #181 — Cosine Score: 0.4908\n",
            "  147. Chunk #92 — Cosine Score: 0.4904\n",
            "  148. Chunk #105 — Cosine Score: 0.4848\n",
            "  149. Chunk #213 — Cosine Score: 0.4839\n",
            "  150. Chunk #94 — Cosine Score: 0.4827\n",
            "  151. Chunk #131 — Cosine Score: 0.4822\n",
            "  152. Chunk #210 — Cosine Score: 0.4820\n",
            "  153. Chunk #45 — Cosine Score: 0.4763\n",
            "  154. Chunk #195 — Cosine Score: 0.4736\n",
            "  155. Chunk #91 — Cosine Score: 0.4725\n",
            "  156. Chunk #207 — Cosine Score: 0.4724\n",
            "  157. Chunk #206 — Cosine Score: 0.4700\n",
            "  158. Chunk #76 — Cosine Score: 0.4679\n",
            "  159. Chunk #0 — Cosine Score: 0.4657\n",
            "  160. Chunk #49 — Cosine Score: 0.4645\n",
            "  161. Chunk #73 — Cosine Score: 0.4622\n",
            "Reduced context char length: 108619\n",
            "Reduced context saved to 'reduced_context.txt'\n",
            "Context reduced by 23.32%\n"
          ]
        }
      ],
      "source": [
        "# Semantic Chunk Ranking and Context Reduction Cell\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "from torch.nn.functional import cosine_similarity\n",
        "import json\n",
        "\n",
        "# 2) Split and filter context into chunks\n",
        "context_chunks = context.split(\"\\n\\n\")\n",
        "print(f\"Total chunks: {len(context_chunks)}\")\n",
        "\n",
        "min_words = 20\n",
        "filtered_chunks = [ch for ch in context_chunks if len(ch.split()) >= min_words]\n",
        "print(f\"Chunks ≥ {min_words} words: {len(filtered_chunks)}\")\n",
        "\n",
        "# 3) Compute semantic embeddings for chunks and questions\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "print(\"Computing chunk embeddings...\")\n",
        "chunk_embeds = embedder.encode(filtered_chunks, convert_to_tensor=True, show_progress_bar=True)\n",
        "\n",
        "print(\"Computing question embeddings...\")\n",
        "question_texts = [q[\"question\"] for q in all_questions]\n",
        "question_embeds = embedder.encode(question_texts, convert_to_tensor=True, show_progress_bar=True)\n",
        "\n",
        "# 4) Aggregate question embedding (mean pooling)\n",
        "query_embed = torch.mean(question_embeds, dim=0)\n",
        "\n",
        "# 5) Compute cosine similarities and rank chunks\n",
        "sims = cosine_similarity(\n",
        "    query_embed.unsqueeze(0).repeat(len(chunk_embeds), 1),\n",
        "    chunk_embeds,\n",
        "    dim=1\n",
        ")\n",
        "#######################\n",
        "top_k=161\n",
        "#######################\n",
        "top_indices = torch.topk(sims, k=top_k).indices.tolist()\n",
        "\n",
        "print(f\"Selected top {top_k} chunks (by semantic relevance):\")\n",
        "for rank, idx in enumerate(top_indices, 1):\n",
        "    print(f\"  {rank}. Chunk #{idx} — Cosine Score: {sims[idx]:.4f}\")\n",
        "\n",
        "# 6) Build reduced context and save\n",
        "reduced_chunks = [filtered_chunks[i] for i in top_indices]\n",
        "reduced_context = \"\\n\\n\".join(reduced_chunks)\n",
        "print(f\"Reduced context char length: {len(reduced_context)}\")\n",
        "with open(\"reduced_context.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(reduced_context)\n",
        "print(\"Reduced context saved to 'reduced_context.txt'\")\n",
        "\n",
        "# 7) Report reduction percentage\n",
        "orig_len = len(context)\n",
        "new_len = len(reduced_context)\n",
        "print(f\"Context reduced by {(1 - new_len / orig_len) * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1502acd",
      "metadata": {},
      "source": [
        "# Pipeline mit dem reduzierten Context-file ausführen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "05043b5e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "78\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\question_answering.py:390: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Question: What does judo mean?\n",
            "   Expected: gentle way         Received: gentle way\n",
            "   Span: 'gentle way' (start: 27, end: 37)\n",
            "\n",
            "   Similarity Score: 100.0\n",
            "\n",
            "✅ Question: Who founded judo?\n",
            "   Expected: Kanō Jigorō         Received: Jigoro Kano\n",
            "   Span: 'Jigoro Kano' (start: 22129, end: 22140)\n",
            "\n",
            "   Similarity Score: 99.37\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \t\treduced_context \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(all_questions))\n\u001b[1;32m----> 6\u001b[0m ACC_ReducedContext \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_qa\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_questions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# <- data (list of QA dicts)\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreduced_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# <- context string\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# <- your pipeline/callable\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTHRESHOLD\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# <- int threshold\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[4], line 18\u001b[0m, in \u001b[0;36mevaluate_qa\u001b[1;34m(data, context, nlp_callable, threshold)\u001b[0m\n\u001b[0;32m     15\u001b[0m expected \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     16\u001b[0m QA_input \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: question, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: context}  \u001b[38;5;66;03m# model input\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mnlp_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQA_input\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# run QA pipeline\u001b[39;00m\n\u001b[0;32m     19\u001b[0m pred \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]           \u001b[38;5;66;03m# predicted answer\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Extract answer span from result\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\question_answering.py:397\u001b[0m, in \u001b[0;36mQuestionAnsweringPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args_parser(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(examples, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(examples) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(examples, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\base.py:1371\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[1;32m-> 1371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1377\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:269\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[1;32m--> 269\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    271\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\base.py:1286\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1284\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1285\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1286\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1287\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\question_answering.py:524\u001b[0m, in \u001b[0;36mQuestionAnsweringPipeline._forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(model_forward)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    523\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 524\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m: output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_logits\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m: output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend_logits\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample\u001b[39m\u001b[38;5;124m\"\u001b[39m: example, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs}\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1624\u001b[0m, in \u001b[0;36mRobertaForQuestionAnswering.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1612\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1613\u001b[0m \u001b[38;5;124;03mstart_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1614\u001b[0m \u001b[38;5;124;03m    Labels for position (index) of the start of the labelled span for computing the token classification loss.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1620\u001b[0m \u001b[38;5;124;03m    are not taken into account for computing the loss.\u001b[39;00m\n\u001b[0;32m   1621\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1622\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1624\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1625\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1630\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1631\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1632\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1633\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1634\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1636\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1638\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqa_outputs(sequence_output)\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:978\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m    974\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m    976\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m--> 978\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    987\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    988\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    989\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    990\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    991\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:631\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    620\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    621\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    622\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    628\u001b[0m         output_attentions,\n\u001b[0;32m    629\u001b[0m     )\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 631\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    639\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    641\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:520\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    510\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    517\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    518\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    519\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 520\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "with open('reduced_context.txt', \"r\", encoding=\"utf-8\") as f:\n",
        "\t\treduced_context = f.read()\n",
        "\n",
        "print(len(all_questions))\n",
        "\n",
        "ACC_ReducedContext = evaluate_qa(\n",
        "    all_questions,  # <- data (list of QA dicts)\n",
        "    reduced_context,            # <- context string\n",
        "    nlp,                # <- your pipeline/callable\n",
        "    THRESHOLD           # <- int threshold\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "916f192c",
      "metadata": {},
      "source": [
        "# Finetuning auf Contextfile/Textkorpus\n",
        "## Low-Rank Adaption = LoRA als effiziente Finetune-Methode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "05ff4138",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://anu9rng:****@rb-artifactory.bosch.com/artifactory/api/pypi/python-virtual/simple\n",
            "Requirement already satisfied: transformers>=4.39.0 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.51.3)\n",
            "Requirement already satisfied: peft==0.15.2 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.15.2)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from peft==0.15.2) (1.26.3)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from peft==0.15.2) (23.2)\n",
            "Requirement already satisfied: psutil in c:\\users\\sea4sgm\\appdata\\roaming\\python\\python311\\site-packages (from peft==0.15.2) (6.1.1)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from peft==0.15.2) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from peft==0.15.2) (2.1.2)\n",
            "Requirement already satisfied: tqdm in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from peft==0.15.2) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from peft==0.15.2) (1.6.0)\n",
            "Requirement already satisfied: safetensors in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from peft==0.15.2) (0.5.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from peft==0.15.2) (0.31.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers>=4.39.0) (3.13.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers>=4.39.0) (2023.12.25)\n",
            "Requirement already satisfied: requests in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers>=4.39.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers>=4.39.0) (0.21.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub>=0.25.0->peft==0.15.2) (2023.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface_hub>=0.25.0->peft==0.15.2) (4.12.2)\n",
            "Requirement already satisfied: sympy in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.13.0->peft==0.15.2) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.13.0->peft==0.15.2) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.13.0->peft==0.15.2) (3.1.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\sea4sgm\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->peft==0.15.2) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.13.0->peft==0.15.2) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers>=4.39.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers>=4.39.0) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers>=4.39.0) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers>=4.39.0) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\sea4sgm\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch>=1.13.0->peft==0.15.2) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install \"transformers>=4.39.0\" \"peft==0.15.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "0b249111",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at deepset/roberta-base-squad2 and are newly initialized: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (41613 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d831855468eb40839624b07bc45e59b7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/81 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\SEA4SGM\\AppData\\Local\\Temp\\ipykernel_13600\\2847681032.py:57: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 11:36, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>17.358800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>17.149500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>16.989800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>16.753800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>16.563000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>16.500600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "('lora-roberta-context-finetuned\\\\tokenizer_config.json',\n",
              " 'lora-roberta-context-finetuned\\\\special_tokens_map.json',\n",
              " 'lora-roberta-context-finetuned\\\\vocab.json',\n",
              " 'lora-roberta-context-finetuned\\\\merges.txt',\n",
              " 'lora-roberta-context-finetuned\\\\added_tokens.json',\n",
              " 'lora-roberta-context-finetuned\\\\tokenizer.json')"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "from datasets import Dataset\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "import torch\n",
        "\n",
        "# Load base model and tokenizer\n",
        "model_name = \"deepset/roberta-base-squad2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "\n",
        "# Prepare model for LoRA\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Add LoRA to the model\n",
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"query\", \"value\"],  # Common for Transformer attention\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"  # Works with MLM too\n",
        ")\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "# Load your context data\n",
        "with open(\"complete_context.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    context_text = f.read()\n",
        "\n",
        "# Chunk into smaller samples for training\n",
        "chunk_size = 512\n",
        "tokens = tokenizer(context_text, return_tensors=\"pt\", truncation=False)[\"input_ids\"][0]\n",
        "chunks = [tokens[i:i+chunk_size] for i in range(0, len(tokens), chunk_size) if len(tokens[i:i+chunk_size]) == chunk_size]\n",
        "\n",
        "# Prepare dataset\n",
        "dataset = Dataset.from_dict({\"input_ids\": chunks})\n",
        "def tokenize(batch):\n",
        "    return {\"input_ids\": batch[\"input_ids\"], \"labels\": batch[\"input_ids\"]}\n",
        "dataset = dataset.map(tokenize)\n",
        "\n",
        "# Data collator for masked language modeling\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
        "\n",
        "# Training setup\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./lora-roberta-context-finetuned\",\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=5e-5,\n",
        "    save_total_limit=1,\n",
        "    logging_steps=10,\n",
        "    save_steps=50,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# Train\n",
        "trainer.train()\n",
        "\n",
        "# Save LoRA fine-tuned model\n",
        "model.save_pretrained(\"lora-roberta-context-finetuned\")\n",
        "tokenizer.save_pretrained(\"lora-roberta-context-finetuned\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "580fbd86",
      "metadata": {},
      "source": [
        "# Testen des Modells nach Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "12fc0c0e",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n",
            "c:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\question_answering.py:390: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "❌ Question: What does judo mean?\n",
            "   Expected: gentle way         Received: kappo\n",
            "   Span: 'kappo' (start: 21806, end: 21811)\n",
            "\n",
            "   Similarity Score: 11.86\n",
            "\n",
            "✅ Question: Who founded judo?\n",
            "   Expected: Kanō Jigorō         Received: Jigoro\n",
            "Kano\n",
            "   Span: 'Jigoro\n",
            "Kano' (start: 14727, end: 14738)\n",
            "\n",
            "   Similarity Score: 99.37\n",
            "\n",
            "❌ Question: What is the name of the school Kanō Jigorō established?\n",
            "   Expected: Kōdōkan         Received: Kōdōkan Judo Institute\n",
            "   Span: 'Kōdōkan Judo Institute' (start: 5176, end: 5198)\n",
            "\n",
            "   Similarity Score: 64.07\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m      2\u001b[0m nlp \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion-answering\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlora-roberta-context-finetuned\u001b[39m\u001b[38;5;124m'\u001b[39m, tokenizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlora-roberta-context-finetuned\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m ACC_FT \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_qa\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_questions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# <- data (list of QA dicts)\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# <- context string\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# <- your pipeline/callable\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTHRESHOLD\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# <- int threshold\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[2], line 18\u001b[0m, in \u001b[0;36mevaluate_qa\u001b[1;34m(data, context, nlp_callable, threshold)\u001b[0m\n\u001b[0;32m     15\u001b[0m expected \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     16\u001b[0m QA_input \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: question, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: context}  \u001b[38;5;66;03m# model input\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mnlp_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQA_input\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# run QA pipeline\u001b[39;00m\n\u001b[0;32m     19\u001b[0m pred \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]           \u001b[38;5;66;03m# predicted answer\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Extract answer span from result\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\question_answering.py:397\u001b[0m, in \u001b[0;36mQuestionAnsweringPipeline.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args_parser(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(examples, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(examples) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(examples, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\base.py:1371\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[1;32m-> 1371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1372\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1373\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   1374\u001b[0m                 [inputs], num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[0;32m   1375\u001b[0m             )\n\u001b[0;32m   1376\u001b[0m         )\n\u001b[0;32m   1377\u001b[0m     )\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:269\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[1;32m--> 269\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    271\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\base.py:1286\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1284\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1285\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1286\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1287\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\question_answering.py:524\u001b[0m, in \u001b[0;36mQuestionAnsweringPipeline._forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(model_forward)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    523\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 524\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m: output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_logits\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m: output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend_logits\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample\u001b[39m\u001b[38;5;124m\"\u001b[39m: example, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs}\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1624\u001b[0m, in \u001b[0;36mRobertaForQuestionAnswering.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1612\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1613\u001b[0m \u001b[38;5;124;03mstart_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1614\u001b[0m \u001b[38;5;124;03m    Labels for position (index) of the start of the labelled span for computing the token classification loss.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1620\u001b[0m \u001b[38;5;124;03m    are not taken into account for computing the loss.\u001b[39;00m\n\u001b[0;32m   1621\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1622\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1624\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1625\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1630\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1631\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1632\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1633\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1634\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1636\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1638\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqa_outputs(sequence_output)\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:978\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m    974\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m    976\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m--> 978\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    987\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    988\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    989\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    990\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    991\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:631\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    620\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    621\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    622\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    628\u001b[0m         output_attentions,\n\u001b[0;32m    629\u001b[0m     )\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 631\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    639\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    641\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:520\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    510\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    517\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    518\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    519\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 520\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:447\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    439\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    445\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    446\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 447\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    456\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    457\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:325\u001b[0m, in \u001b[0;36mRobertaSdpaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mforward(\n\u001b[0;32m    314\u001b[0m         hidden_states,\n\u001b[0;32m    315\u001b[0m         attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    320\u001b[0m         output_attentions,\n\u001b[0;32m    321\u001b[0m     )\n\u001b[0;32m    323\u001b[0m bsz, tgt_len, _ \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m--> 325\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    327\u001b[0m \u001b[38;5;66;03m# If this is instantiated as a cross-attention module, the keys and values come from an encoder; the attention\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;66;03m# mask needs to be such that the encoder's padding tokens are not attended to.\u001b[39;00m\n\u001b[0;32m    329\u001b[0m is_cross_attention \u001b[38;5;241m=\u001b[39m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:712\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    710\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_layer(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    711\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 712\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    713\u001b[0m     torch_result_dtype \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    715\u001b[0m     lora_A_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_A\u001b[38;5;241m.\u001b[39mkeys()\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\SEA4SGM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "nlp = pipeline('question-answering', model='lora-roberta-context-finetuned', tokenizer='lora-roberta-context-finetuned')\n",
        "\n",
        "ACC_FT = evaluate_qa(\n",
        "    all_questions,  # <- data (list of QA dicts)\n",
        "    context,            # <- context string\n",
        "    nlp,                # <- your pipeline/callable\n",
        "    THRESHOLD           # <- int threshold\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2413867",
      "metadata": {},
      "source": [
        "# Vergleich der Performances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6ebc759",
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Replace these with your actual accuracy values\n",
        "ACC_FullContext = ACC_FullContext\n",
        "ACC_ReducedContext = ACC_ReducedContext\n",
        "ACC_FT = ACC_FT\n",
        "\n",
        "labels = ['Full Context', 'Reduced Context', 'Fine-Tuned']\n",
        "accuracies = [ACC_FullContext, ACC_ReducedContext, ACC_FT]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(labels, accuracies)\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.ylim(0, 100)\n",
        "plt.title('QA Model Performance Comparison')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "for i, v in enumerate(accuracies):\n",
        "    plt.text(i, v + 1, f\"{v:.1f}%\", ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdfe652a",
      "metadata": {},
      "source": [
        "# Finetuning auf Beispiel-QA Fragen (keine Fragen aus Testset)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Question_Answering_Modeling_Colab.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01f55c26e0b8450584484d5ad23ae412": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4daf2b139de49dc91e8e16e74e6d8db",
            "placeholder": "​",
            "style": "IPY_MODEL_d5fda85a190b442ebb1545775e6fae97",
            "value": "100%"
          }
        },
        "036af88911f0485198d915396c032935": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "140bcd1c2fab49f4bdc75aa1e5afc09d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "140f2935f454450fa40529c0fe20bd79": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1415df0f8d114bdb86ee973db0c87c46": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e9e92d80f664336bf00bf25d00f0ba6",
              "IPY_MODEL_46e51b92d6b34314bfe66514aa02b919",
              "IPY_MODEL_eb062aa044f54f9d98bcdbc672f95388"
            ],
            "layout": "IPY_MODEL_870b7277f78c4f109dfc7533ee142840"
          }
        },
        "17d00c46ced24e15b8513797b2897f65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_140bcd1c2fab49f4bdc75aa1e5afc09d",
            "placeholder": "​",
            "style": "IPY_MODEL_3878a968288141c4a2110e653c4f1f19",
            "value": "100%"
          }
        },
        "1d6712d971dd404d86ad43fee46d0cf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e9e92d80f664336bf00bf25d00f0ba6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65fb543562724014ab103cd61984e0a5",
            "placeholder": "​",
            "style": "IPY_MODEL_374c594081a942608f196a864c411395",
            "value": "100%"
          }
        },
        "24767af4acd64c8a8cf44bebac15b6d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2736e994a9a14ad9a4b2822f9c69f22a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28c01417ac7b4f428093839f600708bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2d0388598134823b4b7289f8e4b1d9b",
            "placeholder": "​",
            "style": "IPY_MODEL_4321ee3b2993487a9f96b75d785747b4",
            "value": "Generating validation split:  95%"
          }
        },
        "2e452f34d128473e9c629c1843e95a4f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3478213e4a96486e99c6da565a28ba1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "35f6797779b14a29a45d6590336922f4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "364d9238806245a9b938af64dedd9bfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e76ab3cc574a479f8df450a3b6d270a5",
              "IPY_MODEL_634e3c1d372140089ba29d4fb2a27bb8",
              "IPY_MODEL_fa83cc47ff2b40578357ecee193efd98"
            ],
            "layout": "IPY_MODEL_88a19cdd26564cfe8c248aabc2f41480"
          }
        },
        "366542d77b0e47d0b35dc7748e1848fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "374c594081a942608f196a864c411395": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3878a968288141c4a2110e653c4f1f19": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "396955d52f014745b72b9ede5df05537": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d5599e8a3d844b5eb065278dba4d81d4",
              "IPY_MODEL_781d26bb56ef4b84b0ce2c52eaeba3b9",
              "IPY_MODEL_d8615fc1ab214808b63a3049e228702a"
            ],
            "layout": "IPY_MODEL_688746e06bbb44eab5826b9f72c08999"
          }
        },
        "3ec089eb55204b7e8c9dc87e99abffc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fbb2b45bba9e4158804a4b721a5f34cc",
              "IPY_MODEL_8d68b5fdf4aa45208738f89db2f2bde7",
              "IPY_MODEL_52320498bd934ce2a626c610240100b4"
            ],
            "layout": "IPY_MODEL_6a153656ab5344caae15aab729f0907b"
          }
        },
        "4321ee3b2993487a9f96b75d785747b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46ae9e4f87474cb6bad707705949f35b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46e51b92d6b34314bfe66514aa02b919": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7644f5d1b65b4695a4bba79ab1874072",
            "max": 11,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_61bb077db6054cf38ba672e3a40077b7",
            "value": 11
          }
        },
        "4b2510ecfe08474f9fb72e13f7415ecb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52320498bd934ce2a626c610240100b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e143da68f6f84cb69c5b59ea021cf9ec",
            "placeholder": "​",
            "style": "IPY_MODEL_6bda2318d720436caef3c0b8fb475a7a",
            "value": " 87575/87599 [00:09&lt;00:00, 11653.02 examples/s]"
          }
        },
        "58e73a8b0d154a769cb71ebaf5fc0492": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5b9e33d278d14687a778ddcbfa972e0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_891799ed1b414b9697ee5e0f0985edd1",
            "placeholder": "​",
            "style": "IPY_MODEL_ab403514b1034ae497354314a4080057",
            "value": " 88/88 [00:52&lt;00:00,  1.98ba/s]"
          }
        },
        "61bb077db6054cf38ba672e3a40077b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "62ed4cb935a34ea58a864d7b31cf3c52": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "634e3c1d372140089ba29d4fb2a27bb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65bb131e20434e61888597e368aec408",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c4c15c9261c444bbf5358fdcaaa1074",
            "value": 2
          }
        },
        "65750f4599544ebbac8516293256d862": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65bb131e20434e61888597e368aec408": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65fb543562724014ab103cd61984e0a5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6870a192e7904ffb8ca4d190db7bd480": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "688746e06bbb44eab5826b9f72c08999": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a153656ab5344caae15aab729f0907b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6aba591339f34056a07279d3b8e24600": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bda2318d720436caef3c0b8fb475a7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7644f5d1b65b4695a4bba79ab1874072": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "781d26bb56ef4b84b0ce2c52eaeba3b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6aba591339f34056a07279d3b8e24600",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e149b176dd374d7fa126a000ff790418",
            "value": 2
          }
        },
        "7c4c15c9261c444bbf5358fdcaaa1074": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7cea8a3f65fc4d73a79147daff0b0509": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7eadd418cc1343fba6bb7e553e7b1b57": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "870b7277f78c4f109dfc7533ee142840": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88a19cdd26564cfe8c248aabc2f41480": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "891799ed1b414b9697ee5e0f0985edd1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c3bb187e75945eba9ed6559822d61bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d68b5fdf4aa45208738f89db2f2bde7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_140f2935f454450fa40529c0fe20bd79",
            "max": 87599,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3478213e4a96486e99c6da565a28ba1c",
            "value": 87599
          }
        },
        "93fbb8f21d0e451984208d77a649ac4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e452f34d128473e9c629c1843e95a4f",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_62ed4cb935a34ea58a864d7b31cf3c52",
            "value": 2
          }
        },
        "ab403514b1034ae497354314a4080057": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aefb3188f9ca4f7c8df24a0e6d1e8aeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46ae9e4f87474cb6bad707705949f35b",
            "max": 10570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_58e73a8b0d154a769cb71ebaf5fc0492",
            "value": 10570
          }
        },
        "b1bc68d5dfec4a2ba62ac17ce389d32e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b235711015e14852a34bc6f0861ad148": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6870a192e7904ffb8ca4d190db7bd480",
            "placeholder": "​",
            "style": "IPY_MODEL_4b2510ecfe08474f9fb72e13f7415ecb",
            "value": " 10049/10570 [00:01&lt;00:00, 5387.34 examples/s]"
          }
        },
        "b2d0388598134823b4b7289f8e4b1d9b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3a9f207bec44965955c01e741b22dfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd15028deff1404e9cfe33b6307c1a55",
            "max": 88,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_366542d77b0e47d0b35dc7748e1848fe",
            "value": 88
          }
        },
        "be78bc860230498385930530d282f791": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0c006c688714557a334027d2454ddd4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf02ce08bc4a40e3b8f20075e52644c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5599e8a3d844b5eb065278dba4d81d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_036af88911f0485198d915396c032935",
            "placeholder": "​",
            "style": "IPY_MODEL_8c3bb187e75945eba9ed6559822d61bf",
            "value": "Extracting data files: 100%"
          }
        },
        "d5fda85a190b442ebb1545775e6fae97": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8615fc1ab214808b63a3049e228702a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0c006c688714557a334027d2454ddd4",
            "placeholder": "​",
            "style": "IPY_MODEL_24767af4acd64c8a8cf44bebac15b6d6",
            "value": " 2/2 [00:00&lt;00:00, 56.67it/s]"
          }
        },
        "d9c6f4c3bf7c4a4cbdf97823872c3216": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd15028deff1404e9cfe33b6307c1a55": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddd7390af8b946bc93b10d2f4d93f740": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfe61f800b6b440ea4b4832d0cfc94c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_17d00c46ced24e15b8513797b2897f65",
              "IPY_MODEL_93fbb8f21d0e451984208d77a649ac4f",
              "IPY_MODEL_e94ab2934f774ff6b94999c14df20f7d"
            ],
            "layout": "IPY_MODEL_ddd7390af8b946bc93b10d2f4d93f740"
          }
        },
        "e143da68f6f84cb69c5b59ea021cf9ec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e149b176dd374d7fa126a000ff790418": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e4daf2b139de49dc91e8e16e74e6d8db": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e76ab3cc574a479f8df450a3b6d270a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7eadd418cc1343fba6bb7e553e7b1b57",
            "placeholder": "​",
            "style": "IPY_MODEL_7cea8a3f65fc4d73a79147daff0b0509",
            "value": "Downloading data files: 100%"
          }
        },
        "e829f9afaf674a25bda9b56979641bf5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e94ab2934f774ff6b94999c14df20f7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be78bc860230498385930530d282f791",
            "placeholder": "​",
            "style": "IPY_MODEL_1d6712d971dd404d86ad43fee46d0cf6",
            "value": " 2/2 [00:00&lt;00:00, 36.79it/s]"
          }
        },
        "eb062aa044f54f9d98bcdbc672f95388": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65750f4599544ebbac8516293256d862",
            "placeholder": "​",
            "style": "IPY_MODEL_f81da4dce8ab4fe4ad1777cb8f8b4320",
            "value": " 11/11 [00:06&lt;00:00,  1.94ba/s]"
          }
        },
        "f252e6576b2f48ba9a28d25294f5f73e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_28c01417ac7b4f428093839f600708bb",
              "IPY_MODEL_aefb3188f9ca4f7c8df24a0e6d1e8aeb",
              "IPY_MODEL_b235711015e14852a34bc6f0861ad148"
            ],
            "layout": "IPY_MODEL_b1bc68d5dfec4a2ba62ac17ce389d32e"
          }
        },
        "f63670d85417494eae0d0761d1d60f49": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_01f55c26e0b8450584484d5ad23ae412",
              "IPY_MODEL_b3a9f207bec44965955c01e741b22dfd",
              "IPY_MODEL_5b9e33d278d14687a778ddcbfa972e0f"
            ],
            "layout": "IPY_MODEL_2736e994a9a14ad9a4b2822f9c69f22a"
          }
        },
        "f81da4dce8ab4fe4ad1777cb8f8b4320": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa83cc47ff2b40578357ecee193efd98": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35f6797779b14a29a45d6590336922f4",
            "placeholder": "​",
            "style": "IPY_MODEL_cf02ce08bc4a40e3b8f20075e52644c1",
            "value": " 2/2 [00:00&lt;00:00, 57.17it/s]"
          }
        },
        "fbb2b45bba9e4158804a4b721a5f34cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e829f9afaf674a25bda9b56979641bf5",
            "placeholder": "​",
            "style": "IPY_MODEL_d9c6f4c3bf7c4a4cbdf97823872c3216",
            "value": "Generating train split: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
